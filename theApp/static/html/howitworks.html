<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="../../favicon.ico">

    <title>IsThisMetastasis</title>

    <!-- Bootstrap core CSS -->
    <link href="../../static/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="../../static/css/cover.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <script src="../../assets/js/ie-emulation-modes-warning.js"></script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

  </head>

  <body>

    <div class="site-wrapper">

      <div class="site-wrapper-inner">

        <div class="cover-container">

          <div class="masthead clearfix">
            <div class="inner">
              <h3 class="masthead-brand">IsThisMetastasis.info</h3>
              <nav>
                <ul class="nav masthead-nav">
                  <li><a href="/index">Classifier</a></li>
                  <li class="active"><a href="#">How It Works</a></li>
                  <li><a href="#">Get In Touch</a></li>
                </ul>
              </nav>
            </div>
          </div>

	  
	  <div style="text-align:left; margin-top:120px;">
	  <!-- CONTENT GOES HERE -->
	  <h1> Is this metastasis? 
	  <img src="../images/normal.jpeg" width="20%"> &emsp;  <img src="../images/metastatic.jpeg" width = "20%"> </h1><br>
	  <img src="../images/normal.jpeg" width="25%" style="float:right;"/>
	  If you're not a pathologist, you have a 50/50 chance of correctly identifying which of the two images above
	  contains metastatic tissue -- the same probability
	  that an untrained computer could correctly classify the two types of tissue.
	  The image on the left is healthy tissue, and the other is metastatic tissue.
	  How would you classify this other image to the right?  It has a similar color and structure to the tissue
	  that you learned was healthy, so you'd probably guess (correctly) that this image also contains healthy
	  tissue.  Is it possible to train a computer to identify these features, and
	  why is this important?

	  <h2> Histology is repetitive </h2>
	  Histology is the study of the microscopic structure of
	  tissue.  It is often used to accurately diagnose cancer and other diseases, and is an essential tool
	  for researchers evaluating new potential cures for these diseases.  There are many steps involved in preparing
	  a histological slide before it can be examined by a histopathologist -- fixing, processing, embedding,
	  sectioning, and staining. <br><br>
	  The automation of these steps has already been solved by
	  a company called HistoWiz that offers automated histology for biomedical researchers, where tissue samples can
	  be processed in as little as
	  3-days.  HistoWiz returns a DZI (Deep Zoom Image) file to these researchers, allowing them to zoom in up to 40x
	  and resolve tissue features on the micron level.<br><br>  It can take a considerable amount of time for researchers
	  to pour over these images and identify regions of interest.  A computer trained to identify these regions would
	  save researchers substantial time and effort, enabling them to accelerate their groundbreaking research.  Companies
	  that offer this service would be the go-to for these researchers, so investment in these techniques is a worthwhile pursuit.
	  
	  <h2> How do we train a computer </h2>
	  There are two steps involved in training a computer to classify images.  First, we need to use algorithms to identify key
	  features within the images that are able to discriminate between our different categories.  The algorithms that quantify the
	  structure and color of an image fall into the category of computer vision.  <br> <br>
	  Once we've decided on a set a features which we think may have discriminating power between different categories of images, we
	  can use these features as inputs to a classifier.  There are many different classifiers to choose from, but they generally fall
	  into one of two categories : supervised learning and unsupervised learning.  If we have a set of images that has already been
	  labeled, and we want to train a computer to recoginize future images that belong under that label, a supervised learning algorithm
	  would be the best choice.  If instead, we want to find unique groups within an unlabeled set of data, an unsupervised classifier would be
	  the way to go. <br> <br>
	  One last note -- there is a powerful technique called deep learning.  Given a large enough dataset, the feature identification
	  and image classification can be learned by a single machine learning algorithm.  These algorithms require enormously large
	  training datasets to achieve accurate classifications,
	  but once trained, they can be efficiently repurposed for different applications using a technique called transfer learning.

	  <!--
	  <h3> Computer vision </h3>
	  Structual features may involve the size and relative location of various shapes (e.g. lines,
	  edges, circles, etc.), wheras color properties look at both the absolute color and the amount of color variation in an image.
	  Some of these features may be more or less useful depending on the application.  For example, the absolute color may not be
	  useful in identifing cars, which come in a variety of colors, wheras stop signs could be identified by looking for octogonal
	  patches of red.  <br>

	  <h3> Training a classifier </h3>
	  <h3> Deep learning </h3>
	  -->
	  
	  <h2> The data </h2>
	  The data I am working with are composed of 6732 individual tissue slide frame images, equally split between tissue
	  classified as normal and tissue classified as metastatic.  Each frame measure 224x224 pixels, and I used a 60/20/20
	  split to divide that data between my train, cross-validation, and test sets.  Some fraction of the images contain very
	  little tissue (i.e. greater than 95% of the slide is white background).  I excluded these slides from my training, cross-validation, and test
	  sets, leaving 6611 total slides to work with.  3962 of these are use for training, 1325 are used for cross-validation,
	  and the remaining 1324 are reserved for testing.  After visually inspecting both the normal
	  and the metastatic tissue images in my training set, I engineered the following computer vision features: blob density,
	  tissue discontinuity, and color compactness.
	  <h3> Blob density </h3>
	  <img src="../images/normal_blobs.jpeg" width="25%" style="float:right;"/>
	  As a particle physicist by training, I noticed that the healthy tissue contains a greater density of "blobs" than
	  the metastatic tissue (I have since learned that these blobs are healthy cell nuclei).
	  I employed a blob finding algorithm using openCV, which
	  searches each image for groups of connected pixels that share a common property.  The parameters of these algorithms
	  can be tuned to search for blobs of different shapes and sizes.  In the image on the right, the yellow circles indicate
	  where a blob has been found.  Because I set up my blob finder to find circular blobs, it misses overlapping nuclei that
	  appear as a single, elongated blob (e.g. look at the right-middle of the image).
	  <h3> Color compactness </h3>
	  <img src="../images/metastatic_cc.jpeg" width="25%" style="float:right;"/>
	  Color compactness is a measure of the color variability in an image.  A k-means clustering
	  algorithm is used to cluster the pixels in an image into a specified number of classes (k), such
	  that the sum of the distances (in color space) of each pixel from its assigned cluster center is
	  minimized.  This sum (with an addition normalization) is used as the color compactness metric.
	  In other words, color compactness is measure of how well an image can be represented using k colors.  The
	  color compactness measure for this project uses k=3.  This is motivated by the H&E stain used in these slides,
	  which generally produces dichromatic images (k=3 is used because there is often some white space in an image).
	  The image on the right shows the color compacted image of a slide containing metastatic tissue.
	  <h3> Tissue discontinuity </h3>
	  <img src="../images/normal_td.jpeg" width="25%" style="float:right;"/>
	  Tissue discontinuity is a measure of the connectedness of tissue in a given slide.  Tissue that is very connected will have
	  a low tissue discontinuity score, and vice-versa for disconnected tissue.  This feature is computed by converting the image to
	  a black-white representation of itself, and then using an edge detection algorithm to find the boundaries between black and white.
	  Tissue that is not very connected tends to have many small patches of white space, and therefore more edges per unit area of white
	  space (than connected tissue).  In the image on the right, the small colored circles show where the algorithm has detected an edge.  
	  <h3> Absolute color </h3>
	  It can be easy to get excited about computer vision techniques and ignore more basic features that have discrimination
	  power between different types of tissue.  The individual R,G,B pixel intensities, averaged
	  over the images as a whole, are very powerful in discriminating between normal and metastatic tissue.
	  <h2> Feature distributions </h2>
	  The plots below show the distributions of these features for the normal and metastatic tissue in the training set.
	  The distributions are normalized to unit area, so for each bin on the horizontal axis, the vertical axis represents the
	  fraction of training set data in that particular bin.
	  <div style="background-color:white;">
	  <img src="../images/blobs.jpg" width="49%"/> <img src="../images/color_compactness.jpg" width="49%"/> <img src="../images/tissue_discontinuity.jpg" width="49%"/>
	  <img src="../images/avg_blue.jpg" width="49%"/> <img src="../images/avg_red.jpg" width="49%"/> <img src="../images/avg_green.jpg" width="49%"/>
	  </div>
	  
	  <h2> The model(s) </h2>
	  <h3> Logistic regression </h3>
	  Many of the feature disributions above show excellent separation between the normal and metastatic tissue (e.g. blob density
	  and the average R/G/B across all pixels).  Based on these distributions, I would expect a linear model, such as logistic
	  regression, to perform reasonably well.  I trained scikit-learn's LogisticRegression classifier with its default parameters
	  on the 3962 slides in my training set.  This classifier achieves 92.5% accuracy, evaluated on the cross-validation set.
	  

	      
	  <!-- CONTENT ENDS HERE -->
	  </div>
	  
<!--	  
          <div class="mastfoot">
            <div class="inner">
              <p>Cover template for <a href="http://getbootstrap.com">Bootstrap</a>, by <a href="https://twitter.com/mdo">@mdo</a>.</p>
            </div>
          </div>
-->
        </div>

      </div>

    </div>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="../../dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>

